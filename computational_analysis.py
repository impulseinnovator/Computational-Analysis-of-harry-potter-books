# -*- coding: utf-8 -*-
"""Computational Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15zBxIY3-KejmqNNYlFy9AaoghwGJKcZK
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
from textblob import TextBlob
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import sentiwordnet as swn
from nltk.tag import pos_tag
from nltk import RegexpParser
from wordcloud import WordCloud
import seaborn as sns
import re
import sklearn
from pylab import rcParams
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
import matplotlib.pyplot as plt
import pandas as pd

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('Neville.csv', sep=",", encoding='Latin-1', names=['text','target'])

df.head()

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('sentiwordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')

lm = WordNetLemmatizer()

def text_transformation(df_col):
    corpus = []
    for item in df_col:
        new_item = re.sub('[^a-zA-Z]',' ',str(item))
        new_item = new_item.lower()
        new_item = new_item.split()
        new_item = [lm.lemmatize(word) for word in new_item if word not in set(stopwords.words('english'))]
        corpus.append(' '.join(str(x) for x in new_item))
    return corpus

corpus = text_transformation(df['text'])

corpus

rcParams['figure.figsize'] = 20,8
word_cloud = ""
for row in corpus:
    for word in row:
        word_cloud+=" ".join(word)
wordcloud = WordCloud(width = 1000, height = 500,background_color ='white',min_font_size = 10).generate(word_cloud)
plt.imshow(wordcloud)

for i in range(len(df)):
  tokenized = sent_tokenize(df.loc[i,'text'])
  for j in tokenized:
     
    # Word tokenizers is used to find the words
    # and punctuation in a string
    wordsList = nltk.word_tokenize(j)
 
    # removing stop words from wordList
    wordsList = [w for w in wordsList if not w in stop_words]
 
    #  Using a Tagger. Which is part-of-speech
    # tagger or POS-tagger.
    tagged = nltk.pos_tag(wordsList)
 
    print(tagged)

# Extracting all Proper Nouns from a text file using nltk
l1= []
for i in range(len(df)):
    token_comment = word_tokenize(df.loc[i,'text'])
    tagged_comment = pos_tag(token_comment)
    l1.append( [(word, tag) for word, tag in tagged_comment if (tag=='JJ')])
    l1.append( [(word, tag) for word, tag in tagged_comment if (tag=='JJR')])
    l1.append( [(word, tag) for word, tag in tagged_comment if (tag=='JJS')])

l1

new_list = []
for i in range(len(l1)):
  a = l1[i]
  for j in range(len(a)):
    b = a[j]
    new_list.append(b[0])

new_list

trait_list = []
for i in new_list:
  if "first"  not in i and 'Know' not in i and 'sure' not in i and 'last' not in i and 'much' not in i and 'little' not in i and 'old' not in i and 'many' not in i and 'next' not in i and 'able' not in i and 'second' not in i and 'common' not in i:
    trait_list.append(i)

from wordcloud import WordCloud, STOPWORDS
comment_words = ''
stopwords = set(STOPWORDS)
 
# iterate through the csv file
for val in trait_list:
     
    # typecaste each val to string
    val = str(val)
 
    # split the value
    tokens = val.split()
     
    # Converts each token into lowercase
    for i in range(len(tokens)):
        tokens[i] = tokens[i].lower()
     
    comment_words += " ".join(tokens)+" "
 
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = stopwords,
                min_font_size = 10).generate(comment_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()



pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# init the sentiment analyzer
sia = SentimentIntensityAnalyzer()

from google.colab import drive
drive.mount('/content/drive')

import csv
total=0
l=[]
with open('output_book7.csv', mode ='r')as file:
   
  # reading the CSV file
  csvFile = csv.reader(file)
  for lines in csvFile:
        print(lines)
        for sentence in lines:
            score = sia.polarity_scores(sentence)["compound"]
            l.append(score)
            total=total+score
            print(f'The sentiment value of the sentence :"{sentence}" is : {score}')

with open('polarity_book7.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(l)
print(l)

print(total)

import csv
total=0
l=[]
with open('final.csv', mode ='r')as file:
   
  # reading the CSV file
  csvFile = csv.reader(file)
  for lines in csvFile:
        print(lines)
        for sentence in lines:
            score = sia.polarity_scores(sentence)["compound"]
            l.append(score)
            total=total+score
            print(f'The sentiment value of the sentence :"{sentence}" is : {score}')

with open('protagonist.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(l)
print(l)

print(total/5656)